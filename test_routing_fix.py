#!/usr/bin/env python3
"""Test the improved LLM-based routing approach."""

import asyncio
from aletheia.llm.router import LLMRouter, TaskContext

async def test_llm_based_routing():
    """Test that physics questions are properly detected by local LLM self-assessment."""
    print("üß† Testing LLM-Based Routing Approach")
    
    router = LLMRouter()
    
    # Test physics questions from the dialogue
    test_cases = [
        {
            "prompt": "—Ä–∞—Å—Å–∫–∞–∂–∏ —Ç–æ–≥–¥–∞ –ø—Ä–æ –∫–≤–∞–Ω—Ç–æ–≤—É—é –∑–∞–ø—É—Ç–∞–Ω–Ω–æ—Å—Ç—å",
            "description": "Quantum entanglement explanation"
        },
        {
            "prompt": "—Å–æ–≥–ª–∞—Å–Ω–æ –û–¢–û, –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –º–æ–∂–µ—Ç –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å—Å—è –º–≥–Ω–æ–≤–µ–Ω–Ω–æ, –µ—ë –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å —ç—Ç–æ —Å–∫–æ—Ä–æ—Å—Ç—å —Å–≤–µ—Ç–∞, –Ω–µ—Ç ?",
            "description": "Relativity theory question"
        },
        {
            "prompt": "–∫–∞–∫ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç ? –æ—Ç–∫—É–¥–∞ –æ–¥–Ω–∞ —á–∞—Å—Ç–∏—Ü–∞ –º–æ–∂–µ—Ç –∑–Ω–∞—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ –¥—Ä—É–≥–æ–π ?",
            "description": "Quantum mechanics mechanism"
        },
        {
            "prompt": "–ø–æ—á–µ–º—É —Ç–∞–∫ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç ? –º–æ–∂–µ—Ç, –µ—Å—Ç—å –µ—â–µ –∏ –¥—Ä—É–≥–∏–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è ?",
            "description": "Physics exceptions question"
        },
        {
            "prompt": "–ø—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?",
            "description": "Simple greeting (should stay local)"
        }
    ]
    
    print("\nüß™ Testing Local LLM Self-Assessment:")
    
    for i, case in enumerate(test_cases, 1):
        print(f"\n{i}. {case['description']}")
        print(f"   Query: \"{case['prompt'][:50]}...\"")
        
        try:
            # Test local LLM self-assessment directly
            task = TaskContext(
                prompt=case['prompt'],
                max_tokens=512,
                conversation_context=None
            )
            
            # Get structured response from local LLM for routing decision
            structured_result = await router.local_llm.generate_structured(
                prompt=case['prompt'],
                max_tokens=256,
                temperature=0.3
            )
            
            confidence = structured_result.get('confidence', 'unknown')
            external_needed = structured_result.get('external_needed', False)
            reasoning = structured_result.get('reasoning', 'No reasoning provided')
            raw_response = structured_result.get('raw_response', 'No raw response')
            
            print(f"   ü§ñ Local LLM Assessment:")
            print(f"      Confidence: {confidence}")
            print(f"      External needed: {external_needed}")
            print(f"      Reasoning: {reasoning}")
            print(f"      Raw response: {raw_response[:200]}...")  # Debug output
            
            # Test actual routing decision
            routing_decision = await router.route_task(task)
            print(f"   üéØ Routing Decision: {routing_decision.value}")
            
            # Evaluate correctness
            is_scientific = i <= 4  # First 4 are scientific
            should_be_external = is_scientific
            is_correct = (routing_decision.value == "external") == should_be_external
            
            status = "‚úÖ CORRECT" if is_correct else "‚ùå INCORRECT"
            print(f"   {status} (Expected: {'external' if should_be_external else 'local'})")
            
        except Exception as e:
            print(f"   ‚ùå ERROR: {e}")

if __name__ == "__main__":
    asyncio.run(test_llm_based_routing()) 