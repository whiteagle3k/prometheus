# Prometheus Framework Architecture

## Overview

Prometheus is an entity-based AI framework with **Fast LLM routing intelligence** and robust cross-LLM context coordination. The architecture provides clean separation between the generic framework core and specific entity implementations, with advanced routing decisions made by a dedicated Fast LLM oracle.

## System Architecture

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Prometheus    â”‚
                    â”‚   Framework     â”‚
                    â”‚     (Core)      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                   â–¼                   â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚Aletheia â”‚         â”‚ Agent B â”‚         â”‚ Agent C â”‚
   â”‚ Entity  â”‚         â”‚ Entity  â”‚         â”‚ Entity  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                    â”‚                    â”‚
        â–¼                    â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Identity     â”‚    â”‚ Identity     â”‚    â”‚ Identity     â”‚
â”‚ Config       â”‚    â”‚ Config       â”‚    â”‚ Config       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Core Framework                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ BaseEntity  â”‚ LocalLLM    â”‚ LLMRouter   â”‚ Memory        â”‚
â”‚ (Generic)   â”‚ (Generic)   â”‚ (Generic)   â”‚ System        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚             â”‚             â”‚             â”‚
        â–¼             â–¼             â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Processing   â”‚ â”‚ Utility   â”‚ â”‚ External  â”‚ â”‚ Vector      â”‚
â”‚ Pipeline    â”‚ â”‚ LLM       â”‚ â”‚ LLM       â”‚ â”‚ Store       â”‚
â”‚             â”‚ â”‚(phi-mini) â”‚ â”‚ Clients   â”‚ â”‚ (ChromaDB)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Entity-Based Architecture

### Core Framework Components (Generic)

#### 1. BaseEntity
- **Purpose**: Abstract base class for all AI entities
- **Responsibilities**: 
  - Identity loading and management
  - Core functionality (think, autonomous_loop, get_status)
  - Framework integration (router, memory, context)
- **Entity contracts**: Entities override `_load_identity()` and specify `IDENTITY_PATH`

#### 2. LocalLLM (Generic)
- **Purpose**: Generic local language model wrapper
- **Clean design**: No entity-specific knowledge
- **System prompts**: Always in English from identity config
- **Response format**: Simple `ANSWER`, `CONFIDENCE`, `REASONING`
- **No routing logic**: Pure text generation

#### 3. LLMRouter (Generic)
- **Purpose**: Intelligent routing between local and external LLMs
- **Responsibilities**: Route decisions, cost optimization, performance tracking
- **Meta-cognitive**: Uses local LLM's confidence to make routing decisions
- **Generic design**: Works with any entity's identity configuration

#### 4. Memory System (Generic)
- **Vector Store**: ChromaDB-based semantic memory
- **User Profiles**: Personal data extraction and storage
- **Context Manager**: Conversation history and running summaries
- **Processing Pipeline**: Configurable text processing modules

### Entity Implementation (Specific)

#### Aletheia Entity
```python
class AletheiaEntity(BaseEntity):
    IDENTITY_PATH = Path(__file__).parent / "identity"
    
    def _load_identity(self) -> Dict[str, Any]:
        # Load aletheia-specific configuration
        # Merge with fallbacks
        # Return complete identity config
```

#### Entity Identity Configuration
```json
{
  "name": "Aletheia",
  "llm_instructions": "You are Aletheia, a female autonomous technical agent...",
  "personality": {...},
  "module_paths": {
    "local_model_gguf": "models/Phi-3-medium-4k-instruct-Q4_K_M.gguf",
    "utility_model_gguf": "models/phi-3-mini-3.8b-q4_k.gguf"
  },
  "translations": {
    "ru": { ... }
  }
}
```

## Core Principles

### 1. Clean Separation of Concerns
- **Core framework**: Generic, reusable components
- **Entity implementations**: Specific personalities and behaviors
- **No coupling**: Core components don't know about specific entities
- **Dependency injection**: Identity config passed to core components

### 2. English-First System Design
- **System prompts**: Always in English for consistency
- **Internal communication**: Framework operates in English
- **User responses**: Generated in user's preferred language
- **Translation layer**: Handled at entity level through identity config

### 3. Generic Component Design
- **LocalLLM**: Works with any identity configuration
- **Router**: Entity-agnostic routing decisions
- **Memory**: Generic storage and retrieval
- **Processing**: Configurable patterns and filters

## Dual-Model Architecture

### Utility Model (phi-3-mini)
- **Purpose**: Fast classifications and utility tasks
- **Performance**: 60-140ms response times
- **Operations**: Query categorization, memory filtering, concept expansion
- **Independence**: Zero context pollution from main model

### Main Model (phi-3-medium)
- **Purpose**: Complex reasoning and response generation
- **Performance**: 1-3s for thoughtful responses
- **Operations**: Natural conversation, problem solving, self-assessment
- **Context**: Enhanced with utility model insights

### Routing Strategy
1. **Query Analysis**: Utility model categorizes the request
2. **Memory Retrieval**: Semantic filtering and relevance scoring
3. **Self-Assessment**: Main model evaluates its competence
4. **Routing Decision**: Router chooses local vs external processing
5. **Response Generation**: Clean, professional output

## Data Processing Architecture

### Processing Pipeline (Configuration-Driven)
```
core/processing/
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ contamination_filter.json
â”‚   â”œâ”€â”€ complexity_detector.json
â”‚   â”œâ”€â”€ user_data_extractor.json
â”‚   â”œâ”€â”€ reference_detector.json
â”‚   â””â”€â”€ ... (other pattern configs)
â”œâ”€â”€ pipeline.py
â”œâ”€â”€ extractors.py
â”œâ”€â”€ detectors.py
â”œâ”€â”€ filters.py
â””â”€â”€ cleaners.py
```

### Clean Response Pipeline
```
Raw Response â†’ Contamination Filter â†’ Field Removal â†’ Context Fallback â†’ Clean Output
  Structured     Remove Markers       Clean Fields    Topic Preserve    User Ready
```

## File Structure

```
prometheus/
â”œâ”€â”€ core/                      # Generic framework components
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_entity.py         # Abstract base class
â”‚   â”œâ”€â”€ config.py              # Global configuration
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ local_llm.py       # Generic local LLM wrapper
â”‚   â”‚   â”œâ”€â”€ utility_llm.py     # Fast utility model
â”‚   â”‚   â”œâ”€â”€ router.py          # Intelligent routing
â”‚   â”‚   â””â”€â”€ external_llm.py    # External API clients
â”‚   â”œâ”€â”€ memory/
â”‚   â”‚   â”œâ”€â”€ vector_store.py    # ChromaDB wrapper
â”‚   â”‚   â”œâ”€â”€ controller.py      # Memory management
â”‚   â”‚   â”œâ”€â”€ user_profile_store.py
â”‚   â”‚   â””â”€â”€ summariser.py
â”‚   â”œâ”€â”€ context/
â”‚   â”‚   â””â”€â”€ context_manager.py # Conversation context
â”‚   â””â”€â”€ processing/            # Text processing modules
â”‚       â”œâ”€â”€ configs/           # JSON pattern configurations
â”‚       â”œâ”€â”€ pipeline.py
â”‚       â”œâ”€â”€ extractors.py
â”‚       â”œâ”€â”€ detectors.py
â”‚       â”œâ”€â”€ filters.py
â”‚       â””â”€â”€ cleaners.py
â”œâ”€â”€ entities/                  # Entity implementations
â”‚   â”œâ”€â”€ __init__.py           # Entity registry
â”‚   â””â”€â”€ aletheia/             # Aletheia entity
â”‚       â”œâ”€â”€ __init__.py       # AletheiaEntity class
â”‚       â””â”€â”€ identity/
â”‚           â””â”€â”€ identity.json # Aletheia configuration
â”œâ”€â”€ prometheus.py             # CLI interface
â”œâ”€â”€ tests/                    # Test suite
â”œâ”€â”€ docs/                     # Documentation
â””â”€â”€ models/                   # Model storage
    â”œâ”€â”€ Phi-3-medium-4k-instruct-Q4_K_M.gguf
    â””â”€â”€ phi-3-mini-3.8b-q4_k.gguf
```

## Performance Characteristics

### Response Times
- **User Data Queries**: 0ms (instant from profile storage)
- **Utility Operations**: 60-140ms (classifications, filtering)
- **Local Processing**: 1-3s (conversation, reasoning)
- **External Consultation**: 3-8s (for complex/scientific queries)

### Resource Usage
- **Memory**: ~8GB RAM for dual-model operation
- **Storage**: ~10GB for models and data
- **GPU**: Metal acceleration on Apple Silicon
- **Cost**: 85% local processing, 15% external API calls

### Routing Efficiency
- **Local Processing**: 85% of interactions
- **External Consultation**: 15% for accuracy-critical tasks
- **Context Pollution**: 0% (independent utility operations)
- **Speed Improvement**: 20x faster classifications

## Multi-language Support

### Architecture
- **System Level**: English-only for consistency
- **Entity Level**: Multilingual identity configurations
- **Response Level**: User's preferred language
- **Translation**: Handled through entity identity config

### Example (Russian Support)
```json
{
  "llm_instructions": "You are Aletheia, a female autonomous technical agent. When responding in Russian, always use feminine language forms: Ð³Ð¾Ñ‚Ð¾Ð²Ð° (not Ð³Ð¾Ñ‚Ð¾Ð²), Ñ€Ð°Ð´Ð° (not Ñ€Ð°Ð´)...",
  "translations": {
    "ru": {
      "greeting_templates": {
        "casual": "ÐŸÑ€Ð¸Ð²ÐµÑ‚! ÐšÐ°Ðº Ð´ÐµÐ»Ð°?",
        "professional": "Ð—Ð´Ñ€Ð°Ð²ÑÑ‚Ð²ÑƒÐ¹Ñ‚Ðµ! Ð¯ {name}, Ð³Ð¾Ñ‚Ð¾Ð²Ð° Ð¿Ð¾Ð¼Ð¾Ñ‡ÑŒ."
      }
    }
  }
}
```

## Hardware Requirements

### Minimum
- **RAM**: 16GB
- **Storage**: 20GB free space
- **CPU**: Apple Silicon M1/M2/M3 or equivalent
- **GPU**: Metal acceleration support

### Recommended
- **RAM**: 32GB+ for optimal dual-model performance
- **Storage**: 50GB+ for extended memory and models
- **Network**: Stable internet for external LLM access 

## Architecture Guide

### Core Architecture Principles

#### 1. Entity-Based Design
- **Entities**: Autonomous AI agents with their own identities, personalities, and configurations
- **Generic Core**: Framework components work with any entity configuration
- **Clean Separation**: No coupling between core framework and specific entities
- **Identity Injection**: Entity configurations passed to generic components

#### 2. Fast LLM Routing Intelligence
- **Unbiased Oracle**: Dedicated Fast LLM (phi-3-mini) makes routing decisions
- **Context Isolation**: Each routing decision is completely independent
- **Smart Classification**: LOCAL vs EXTERNAL based on query complexity
- **Zero Contamination**: No context leakage between routing evaluations

#### 3. Cross-LLM Context Coordination
- **Seamless Flow**: Clean context passing between all LLM components
- **Memory Integration**: Context preserved across conversations and sessions
- **Profile Continuity**: User data flows correctly through all components
- **Clean Preparation**: Focused context extraction for external consultations

### Enhanced Architecture with Fast LLM Routing

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PROMETHEUS FRAMEWORK                         â”‚
â”‚                                                                 â”‚
â”‚   User Query                                                    â”‚
â”‚       â”‚                                                         â”‚
â”‚       â–¼                                                         â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚ â”‚   Fast LLM       â”‚â”€â”€â”€â–¶â”‚   LLM Router        â”‚               â”‚
â”‚ â”‚   (phi-3-mini)   â”‚    â”‚   (Decision Maker)  â”‚               â”‚
â”‚ â”‚   â€¢ Independent  â”‚    â”‚                     â”‚               â”‚
â”‚ â”‚   â€¢ Context-Free â”‚    â”‚   LOCAL â†â†’ EXTERNAL â”‚               â”‚
â”‚ â”‚   â€¢ Unbiased     â”‚    â”‚                     â”‚               â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                   â”‚                            â”‚
â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚                     â–¼             â–¼             â–¼             â”‚
â”‚             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚             â”‚   Local LLM  â”‚ â”‚ External LLM â”‚ â”‚   Memory     â”‚â”‚
â”‚             â”‚  (Phi-3-M)   â”‚ â”‚  (OpenAI)    â”‚ â”‚   System     â”‚â”‚
â”‚             â”‚              â”‚ â”‚              â”‚ â”‚              â”‚â”‚
â”‚             â”‚ Context âœ“    â”‚ â”‚ Context âœ“    â”‚ â”‚ Context âœ“    â”‚â”‚
â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                   â”‚                            â”‚
â”‚                                   â–¼                            â”‚
â”‚                             Clean Response                     â”‚
â”‚                          (No Contamination)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Components

#### Fast LLM (Routing Oracle)
**Purpose**: Make unbiased routing decisions without content generation bias

**Key Features**:
- **Independent Operation**: No coupling with content generation
- **Context Isolation**: Each decision made with clean state
- **High Performance**: 12 GPU layers, 2048 context, optimized for speed
- **Robust Fallbacks**: Rule-based routing when model unavailable

**Configuration**:
```json
{
  "utility_performance_config": {
    "gpu_layers": 12,
    "context_size": 2048,
    "batch_size": 256,
    "threads": 4
  }
}
```

**Routing Decision Process**:
1. **Context Reset**: Model state cleared to prevent contamination
2. **Clean Analysis**: Query analyzed independently of previous decisions
3. **Decision Output**: JSON with route, confidence, reasoning, complexity
4. **Validation**: Response validated and fallback triggered if needed

#### LLM Router
**Purpose**: Coordinate between Fast LLM decisions and actual LLM execution

**Routing Logic**:
```python
# 1. Basic threshold checks
if estimated_tokens > routing_threshold:
    return EXTERNAL

# 2. Fast LLM routing decision
routing_result = await fast_llm.make_routing_decision(query, clean_context)
if routing_result['route'] == 'EXTERNAL':
    return EXTERNAL
else:
    return LOCAL

# 3. Fallback to rule-based routing
```

**Context Preparation**:
- **For Local LLM**: Clean context with user data and conversation history
- **For External LLM**: Structured consultation request with entity personality
- **For Memory**: Semantic filtering and relevance scoring

#### Local LLM
**Purpose**: Generate responses for routine conversations and simple questions

**Key Features**:
- **Generic Design**: Works with any entity identity configuration
- **Feminine Forms**: Proper Russian language forms for female entities
- **Context-Aware**: Integrates conversation history and user profiles
- **Clean Output**: Structured response parsing without contamination

**Generation Process**:
1. **Identity Integration**: Uses entity's llm_instructions and personality
2. **Language Detection**: Automatically detects user language preference
3. **Context Assembly**: Combines conversation history, user profile, current query
4. **Response Generation**: Produces ANSWER, CONFIDENCE, REASONING format
5. **Clean Parsing**: Removes technical markers from user-facing response

#### External LLM Manager
**Purpose**: Handle consultation with external LLM providers (OpenAI, Anthropic)

**Consultation Process**:
1. **Context Preparation**: Build comprehensive consultation request
2. **Provider Selection**: Choose best available external provider
3. **Structured Request**: Send request with entity identity and context
4. **Response Parsing**: Extract technical analysis, user response, memory points
5. **Integration**: Return clean response with consultation metadata

#### Memory System
**Purpose**: Store and retrieve conversation history, user profiles, and context

**Three-Tier Architecture**:
- **Core-Self**: Entity's own memories and learning
- **User**: User-specific profiles and conversation history  
- **Environment**: General knowledge and external information

**Context Flow**:
- **Input**: Semantic classification and relevance filtering
- **Storage**: Vector embeddings with metadata
- **Retrieval**: Context-aware memory selection for LLM consumption

### Context Handling Architecture

#### Context Isolation Strategy
The biggest challenge in cross-LLM environments is **context contamination** - where previous queries influence routing decisions or response generation. Our solution:

##### 1. Fast LLM Context Reset
```python
async def _reset_model_context(self) -> None:
    """Reset model context to prevent contamination."""
    try:
        # Method 1: Explicit reset if available
        if hasattr(self.model, 'reset'):
            self.model.reset()
            return
            
        # Method 2: Clear internal state
        if hasattr(self.model, '_ctx'):
            setattr(self.model, '_ctx', None)
            return
            
        # Method 3: Force context separation
        self.model("<|system|>Clear context<|end|>", max_tokens=1)
    except Exception as e:
        print(f"âš ï¸ Could not reset utility model context: {e}")
```

##### 2. Clean Context Preparation
For external LLM consultations, we prepare focused context:
```python
def _prepare_clean_context(self, conversation_context: str) -> str:
    """Extract only relevant context without contamination."""
    context_lines = conversation_context.strip().split('\n')
    
    # Take last 3-4 exchanges maximum
    relevant_lines = []
    for line in context_lines[-8:]:
        if (line.strip() and 
            not line.startswith('[') and 
            len(line) < 150):
            relevant_lines.append(line.strip())
    
    # Limit to 300 chars max for routing context
    clean_context = '\n'.join(relevant_lines[-4:])
    if len(clean_context) > 300:
        clean_context = clean_context[-300:]
    
    return clean_context
```

##### 3. Independent Decision Making
Each component operates independently:
- **Fast LLM**: Makes routing decisions without knowledge of content
- **Local LLM**: Generates responses without knowledge of routing logic
- **External LLM**: Receives structured consultation requests
- **Memory**: Stores and retrieves context without routing bias

#### Context Flow Diagram

```
User Query
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Fast LLM       â”‚ â† Clean, isolated routing decision
â”‚  (Context-Free) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼ (Route Decision)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM Router     â”‚ â† Coordinates but doesn't influence content
â”‚  (Orchestrator) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼ (With Full Context)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Content LLM    â”‚ â† Receives clean context for generation
â”‚  (Local/External)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Memory System  â”‚ â† Stores result with proper context
â”‚  (Persistent)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Entity Development

### Creating New Entities

1. **Entity Class**: Inherit from `BaseEntity`
```python
class MyAgentEntity(BaseEntity):
    IDENTITY_PATH = Path(__file__).parent / "identity"
    
    def _load_identity(self) -> Dict[str, Any]:
        # Load your entity's specific configuration
        return identity_config
```

2. **Identity Configuration**: Create `identity/identity.json`
```json
{
  "name": "MyAgent",
  "llm_instructions": "You are MyAgent, a specialized AI assistant...",
  "personality": {
    "summary": "Specialized assistant for domain X"
  },
  "module_paths": {
    "local_model_gguf": "models/your-model.gguf",
    "utility_model_gguf": "models/phi-3-mini-3.8b-q4_k.gguf",
    "utility_performance_config": {
      "gpu_layers": 12,
      "context_size": 2048
    }
  }
}
```

3. **Specialized Components**: Override core components if needed
```python
async def _create_specialized_router(self) -> LLMRouter:
    """Create router with entity-specific configuration."""
    return LLMRouter(identity_config=self.identity_config)
```

### Best Practices

1. **Keep Core Generic**: Don't add entity-specific logic to core components
2. **Use Identity Configuration**: Pass all entity specifics through configuration
3. **Preserve Context Flow**: Ensure clean context passing in custom components
4. **Test Routing**: Validate that Fast LLM routing works correctly
5. **Monitor Contamination**: Watch for context leakage between routing decisions

## Performance Optimization

### Model Configuration
- **Fast LLM**: 12 GPU layers, 2048 context (optimized for speed)
- **Local LLM**: 40 GPU layers, 8192 context (optimized for quality)
- **Memory**: Vector search with semantic filtering

### Context Management
- **Routing Context**: Limited to 300 characters
- **Generation Context**: Full conversation history with user profile
- **Memory Context**: Semantic relevance filtering

### Monitoring
```python
# Debug output shows performance metrics
ðŸ’­ Total: 13.3s | LLM: 10.5s | No context contamination
ðŸ”§ Fast LLM routing: LOCAL (confidence: high, complexity: simple)
ðŸ“‚ Found 2 memories, filtering for relevance...
```

## Troubleshooting

### Context Contamination Issues
**Symptoms**: Wrong routing decisions based on previous queries
**Solution**: Verify Fast LLM context reset is working properly

### Routing Performance Issues  
**Symptoms**: Slow routing decisions
**Solution**: Check utility model GPU layers and context size

### Memory Integration Problems
**Symptoms**: Context not preserved across conversations
**Solution**: Verify memory system initialization and context flow

See [Troubleshooting Guide](troubleshooting.md) for detailed debugging information 