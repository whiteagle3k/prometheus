#!/usr/bin/env python3
"""Debug local LLM responses for scientific questions."""

import asyncio
from aletheia.llm.router import LLMRouter
from aletheia.processing.filters import ContaminationFilter

async def debug_llm_responses():
    print("üîç Debugging Local LLM Self-Assessment")
    
    router = LLMRouter()
    
    test_question = "—Ä–∞—Å—Å–∫–∞–∂–∏ —Ç–æ–≥–¥–∞ –ø—Ä–æ –∫–≤–∞–Ω—Ç–æ–≤—É—é –∑–∞–ø—É—Ç–∞–Ω–Ω–æ—Å—Ç—å"
    print(f"Question: {test_question}")
    
    # Get raw response first
    raw_response = await router.local_llm.generate(
        prompt=router.local_llm._format_structured_prompt(test_question),
        max_tokens=256,
        temperature=0.3
    )
    
    print("\nüîç Raw Response (before filtering):")
    print(raw_response)
    
    # Test contamination filter
    contamination_filter = ContaminationFilter()
    filter_result = contamination_filter.process(raw_response)
    filtered_response = filter_result.data if filter_result.success else raw_response.strip()
    
    print("\nüßπ After Contamination Filter:")
    print(filtered_response)
    
    # Test full structured parsing
    result = await router.local_llm.generate_structured(
        prompt=test_question,
        max_tokens=256,
        temperature=0.3
    )
    
    print("\nüìù Final Structured Result:")
    print(f"Answer: {result.get('answer', 'None')}")
    print(f"Confidence: {result.get('confidence', 'None')}")
    print(f"External needed: {result.get('external_needed', 'None')}")
    print(f"Reasoning: {result.get('reasoning', 'None')}")

if __name__ == "__main__":
    asyncio.run(debug_llm_responses()) 